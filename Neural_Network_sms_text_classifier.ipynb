{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "   !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "#from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "train_filepath = \"train-data.tsv\"\n",
        "test_filepath = \"valid-data.tsv\"\n",
        "\n",
        "train_df = pd.read_csv(train_filepath, sep=\"\\t\", header=None, names=[\"type\", \"msg\"])\n",
        "train_df.dropna() #Remove rows with missing values\n",
        "train_df.head() #Display the first 5 data\n",
        "\n",
        "# Convert and read tsv into a Data Frame\n",
        "# sep=\"\\t\": Indicates that the values in the file are separated by tabs (\\t).\n",
        "# header=None: Specifies that the file doesn't have a header row.\n",
        "# names=[\"type\", \"msg\"]: Assigns column names \"type\" and \"msg\" to the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(test_filepath, sep=\"\\t\", header=None, names=[\"type\", \"msg\"])\n",
        "test_df.dropna()\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"type\"] = pd.factorize(train_df[\"type\"])[0]\n",
        "test_df[\"type\"] = pd.factorize(test_df[\"type\"])[0]\n",
        "train_df.head()\n",
        "\n",
        "# COnverts the Text Labels into Numerical Labels: the labels in 'type' column which are 'spam' and 'ham' are numercially respresented(1 and 0, respectfully)\n",
        "# pd.factorize(): This function takes a sequence of values (like the 'type' column) and converts them into unique numerical codes. It returns two arrays: one with the numerical codes and another with the unique values that were encoded."
      ],
      "metadata": {
        "id": "pMx7lv6wT4yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels =  train_df[\"type\"].values #Selecting and extracting 'type' column in DataFrame into train_labels; .values() makes the data compatible to tensorflow\n",
        "train_ds = tf.data.Dataset.from_tensor_slices( #takes slices of the input data and creates individual elements for the dataset.\n",
        "    (train_df[\"msg\"].values, train_labels) #This line creates a TensorFlow Dataset object called train_ds using the message text and corresponding labels\n",
        ")"
      ],
      "metadata": {
        "id": "iOpVBMECU5Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels =  test_df[\"type\"].values\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df[\"msg\"].values, test_labels)\n",
        ")\n",
        "test_ds.element_spec"
      ],
      "metadata": {
        "id": "ftIcim3GU9V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 100 #Used for shuffling and training; larger buffer allows better randomization but consumes more memory\n",
        "BATCH_SIZE = 32\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#Shuffling: shuffles/randomizes the training data to improve model generalization\n",
        "#Batches: Divides the shuffled data into batches of specified size(efficient processing)\n",
        "#Prefetching:  prefetches data for the next batch while the current batch is being processed, minimizing idle time and speeding up training."
      ],
      "metadata": {
        "id": "M4P_uwnSWUR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TextVectorization( # A tool specifically designed to convert text into numbers.\n",
        "    output_mode='int', #Unique int\n",
        "    max_tokens=1000,\n",
        "    output_sequence_length=1000,\n",
        ")\n",
        "\n",
        "vec.adapt(train_ds.map(lambda text, label: text))#extracts the text data from your train_ds dataset, ignoring the labels\n",
        "\n",
        "# Where the TextVectorization layer learns from your training data.\n",
        "# The TextVectorization layer analyzes the text and builds a vocabulary of the most frequent words. It learns how to map each word to a unique integer."
      ],
      "metadata": {
        "id": "ywhrMEUaXxhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(vec.get_vocabulary()) # Retrieves the vocabulary that the TextVectorization layer has built\n",
        "vocab[:20]"
      ],
      "metadata": {
        "id": "qWt2pNsXX5cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vec.get_vocabulary())"
      ],
      "metadata": {
        "id": "8yUUUCxXcLGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([ #Neural Network MOdel\n",
        "    vec,#layer that turns text into unique numerical data\n",
        "    tf.keras.layers.Embedding(#Transforms numerical representation into dense vectors that represent the meaning(similiar words have vectors that are closer)\n",
        "        len(vec.get_vocabulary()),\n",
        "        64, # Dimensionality of the dense vectors\n",
        "        mask_zero=False,\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), #Processes the sequence of word embeddings in forward and backward directions, capturing contextual info\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(1000, activation='relu'), #1000 Neurons, ReLU activation func\n",
        "    tf.keras.layers.Dense(500, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3), #30% dropout of neurons\n",
        "    tf.keras.layers.Dense(1) #Produced score/probability for classification ('spam' or 'ham')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "sZM1-E25Z_iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    validation_steps=30,\n",
        "    epochs=10,\n",
        ")\n",
        "# The model's performance is evaluated on the validation dataset after each epoch.\n",
        "# This helps monitor how well the model is generalizing to unseen data during training."
      ],
      "metadata": {
        "id": "3L2RZRcscJF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "esNmjDDwRgQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy' , test_acc)"
      ],
      "metadata": {
        "id": "9N2QF_WiRm1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim(None, 1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim(0, None)"
      ],
      "metadata": {
        "id": "E12nSF1dSYw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = history.history\n",
        "print(h['loss'])\n",
        "print(h['val_loss'])\n",
        "print(h['accuracy'])\n",
        "print(h['val_accuracy'])\n",
        "\n",
        "#Loss, Val_Loss, Accuracy, Val_Accuracy over the last 10 epoches"
      ],
      "metadata": {
        "id": "od4xoXpUUSFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "# function to predict messages based on model\n",
        "# (Returning prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "\n",
        "def predict_message(pred_text):\n",
        "  # # Convert pred_text to a tf.data.Dataset\n",
        "  pred_text_ds = tf.data.Dataset.from_tensor_slices([pred_text])\n",
        "  # # Adapt the TextVectorization layer to the new dataset\n",
        "  pred_text_ds = pred_text_ds.batch(1).prefetch(tf.data.AUTOTUNE)\n",
        "  pred = model.predict(pred_text_ds)\n",
        "  print(pred)\n",
        "  prob = tf.sigmoid(pred[0][0]).numpy() #Sigmoid function \"squashes\" the logit value into range of 0 to 1, making it a probability instead\n",
        "  return [prob, \"ham\" if prob <0.5 else \"spam\"]\n",
        "\n",
        "pred_text = \"Sale today! to stop texts call 98912460324\" #\"how are you doing today?\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {},
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}